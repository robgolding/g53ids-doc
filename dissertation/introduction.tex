\chapter{Introduction}

It is a well-known fact that computers (or more specifically hard drives) do
not last forever. When these parts fail, it can be a very expensive and
time-consuming process to recover the data that was stored on them - not to
mention the cost of operating without said data until it is recovered (that is
if it \emph{can} be recovered). In fact, in the US alone, data loss is
estimated to cost businesses \$18.2 billion annually\cite{smith03}. Therefore,
keeping backups of important or valuable data is an essential part of
maintaining a computer system or network.

There are two main approaches to backing up a file system: physical and
logical\cite{hutchinson99}. Physical file system backups duplicate the physical
medium upon which the data resides. This is analogous to the Unix ``dd''
command, and requires that the backup image be ``mounted'' before individual
files can be extracted. This method is a good fit for disaster recovery
scenarios (when an entire server has failed, for example), but not so for
accidentally deleted or overwritten files.

Logical file system backups, however, aim to interpret the file system meta
data, in order to discover which files need to be duplicated.  In contrast to
the physical method, logical file system backups make the process of restoring
individual files and folders very simple, though they do not lend themselves so
easily to ``bare-metal'' restores.

When referring to the restoration process, there are primarily two types of
restore: disaster recovery and stupidity recovery\cite{hutchinson99}. These
loosely correspond with the two backup approaches discussed earlier (physical
and logical) respectively. Stupidity recovery refers to the request for a small
number of files and/or folders to be recovered, due to accidental deletion or
overwriting.

This project concerns the development of a backup system to address this type
of request (``stupidity recovery'') in a new way.

\section{A Brief History}

When magnetic tapes replaced punch cards as the primary digital storage medium
in the 1960s, their reliability, scalability, and low cost meant that people
could afford to keep backup copies of their data. Even today, magnetic tapes
are used as a viable method of storing backups.

In the mid 1980s, hard drives started to become a feasible alternative to
magnetic tapes for archiving data. Until this point, they had simply been too
expensive, and too small to be suitable.

During this time, floppy disks also made a contribution to the backup
ecosystem. Though not as ubiquitous as magnetic tapes, the trend towards
smaller disks with larger capacities made them ideal for small-scale backup
operations, beginning in the mid 1970s.

With the introduction of the recordable compact disc in 1990---offering
much more capacity in a similar sized medium---the floppy disk fell into
decline. By the beginning of the new millennium they had all but disappeared,
replaced by rewritable CDs and DVDs.

The latter part of the century also saw the introduction of internet-based
backup providers, offering a hosted service whereby customers' data is
replicated over the internet, and stored---usually on a Storage Area Network
(SAN)---at the provider's data facility. This led to an explosion in the amount
of data being archived on hard disk drives, driven by the ever-growing storage
capacity and falling cost of such disks. These providers are termed ``cloud''
backup providers.

\section{Motivation}

Tradition Version Control Systems (VCS) can be used as a method for backing up
a collection of files, and storing a copy of each and every version as the
files change over time. In practice, however, this is very impractical. Every
time a file changes, the user must ``commit'' the working directory to the
revision control system, which then calculates the changes since the last
commit. Most (if not all) systems use a text-based diffing process, as they are
designed for tracking changes in \emph{source code}. This means that they are
inefficient when used with binary files, such as those that are typically found
in an organisation's file server tree.

This project aims to combine the features of traditional backup solutions with
those of Version Control Systems. The desired result is therefore a system
which keeps a historical archive of every version of every file in a given
directory tree on a remote system. This is something that, combined with an
intuitive interface, is not available in any open-source offerings.

The core feature-set of the system includes:

\begin{itemize}
    \item Version-based file recovery;
    \item Web interface for central management;
    \item Detailed reports regarding backed up data;
    \item Ability to recover files with standard Unix tools (tar, cp, etc.);
    \item Instant file recovery through web interface.
\end{itemize}
