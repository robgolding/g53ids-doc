\chapter{Introduction}

It is a well-known fact that computers (or more specifically hard drives) do
not last forever. When these parts fail, it can be a very expensive and
time-consuming process to recover the data that was stored on them - not to
mention the cost of operating without said data until it is recovered (that is
if it \emph{can} be recovered). In fact, in the US alone, data loss is
estimated to cost businesses \$18.2 billion annually\cite{smith03}. Therefore,
keeping backups of important or valuable data is an essential part of
maintaining a computer system or network.

There are two main approaches to backing up a file system: physical and
logical\cite{hutchinson99}. Physical file system backups duplicate the physical
medium upon which the data resides. This is analogous to the Unix ``dd''
command, and requires that the backup image be ``mounted'' before individual
files can be extracted. This method is a good fit for disaster recovery
scenarios (when an entire server has failed, for example), but not so for
accidentally deleted or overwritten files.

Logical file system backups, however, aim to interpret the file system meta
data, in order to discover which files need to be duplicated.  In contrast to
the physical method, logical file system backups make the process of restoring
individual files and folders very simple, though they do not lend themselves so
easily to ``bare-metal'' restores.

When referring to the restoration process, there are primarily two types of
restore: disaster recovery and stupidity recovery\cite{hutchinson99}. These
loosely correspond with the two backup approaches discussed earlier (physical
and logical) respectively. Stupidity recovery refers to the request for a small
number of files and/or folders to be recovered, due to accidental deletion or
overwriting.

This project concerns the development of a backup system to address this type
of request (``stupidity recovery'') in a new way.

\section{A Brief History}

When magnetic tapes replaced punch cards as the primary digital storage medium
in the 1960s, their reliability, scalability, and low cost meant that people
could afford to keep backup copies of their data. Even today, magnetic tapes
are used as a viable method of storing backups.

In the mid 1980s, hard drives started to become a feasible alternative to
magnetic tapes for archiving data. Until this point, they had simply been too
expensive, and too small to be suitable.

During this time, floppy disks also made a contribution to the backup
ecosystem. Though not as ubiquitous as magnetic tapes, the trend towards
smaller disks with larger capacities made them ideal for small-scale backup
operations, beginning in the mid 1970s.

With the introduction of the recordable compact disc in 1990---offering
much more capacity in a similar sized medium---the floppy disk fell into
decline. By the beginning of the new millennium they had all but disappeared,
replaced by rewritable CDs and DVDs.

The latter part of the century also saw the introduction of internet-based
backup providers, offering a hosted service whereby customers' data is
replicated over the internet, and stored---usually on a Storage Area Network
(SAN)---at the provider's data facility. This led to an explosion in the amount
of data being archived on hard disk drives, driven by the ever-growing storage
capacity and falling cost of such disks. These providers are termed ``cloud''
backup providers.

\subsection{The State of Backups}

There are three main types of backup system in use today, covering the broad
spectrum of usage from home user to commercial company. These are
``traditional'' backup systems, cloud-based backup systems and file
synchronisation services.

File synchronisation services such as Dropbox, which are usually free for
a limited amount of storage, can be used to backup files by synchronising them
with another computer or simply by storing them in the ``cloud''. This type of
service is most likely to be used by home users, due to the absence of any
extra cost.

Cloud-based backup systems such as Mozy, which backup files over the internet
to the service providers data centre, require no additional hardware to be
installed at the client's premises. They often offer a small amount of space
for free, but require a recurring fee to be paid for more realistic storage
requirements. Due to the low maintenance requirements, this type of service is
often used by small to medium-sized businesses.

``Traditional'' backup systems such as Symantec Backup Exec, which usually
store backups on a magnetic tape are installed ``in-house'' on the same network
as the clients themselves. The software commonly requires a licence fee to be
paid, which can often be expensive. For this reason, these such systems are
usually used by larger organisations and corporations, though open-source
alternatives are available which can be used without cost.

\section{Project Aims}

Tradition Version Control Systems (VCS) can be used as a method for backing up
a collection of files, and storing a copy of each and every version as the
files change over time. In practice, however, this is very impractical. Every
time a file changes, the user must ``commit'' the working directory to the
version control system, which then calculates the changes since the last
commit. Most (if not all) systems use a text-based \emph{diffing} process, as
they are designed for tracking changes in source code. This means that they are
inefficient when used with binary files, such as those that are typically found
on a commercial file server.

This project aims to combine the features of traditional backup solutions with
those of modern Version Control Systems. The desired result is therefore
a system which keeps a historical archive of every version of every file in
a given directory tree on a remote, networked system. This is something that,
combined with an intuitive interface, is not available in any open-source
offerings.

The core feature-set of the system includes:

\begin{itemize}
    \item Version-based file backup and recovery;
    \item Web interface for central management;
    \item Detailed reports regarding backed up data;
    \item Instant file recovery through web interface.
\end{itemize}

\subsection{Objectives}

To achieve the project aims, a number of intermediate objectives must be
completed. These include:

\begin{enumerate}
    \item Research the technologies used in similar backup systems by
        evaluating the implementation of a number of related works;
    \item Gain expertise in the usage of the technologies required to implement
        a network backup system;
    \item Implement a prototype system which fulfils the basic backup
        functionality;
    \item Gain industrial support for the project by demonstrating the
        prototype to potential project sponsors;
    \item Specify the details system requirements in collaboration with an
        industrial sponsor, to define the exact system functionality;
    \item Implement the system requirements using the prototype as a base,
        iterating through several revisions to achieve the final result;
    \item Test that the completed system meets the specification
        satisfactorily;
    \item Evaluate the system, comparing it with the desired functionality
        above;
\end{enumerate}
